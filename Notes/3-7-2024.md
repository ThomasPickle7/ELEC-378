# Notes for 3/7/2024

## Support Vector Machines cont'd

Soft Margin: allows data points to be classified within or over the hyperslab.

Hard Margin: does not allow any data points to be classified within the hyperslab.

Misclassified points within the hyperslab have a margin of \( \frac{c}{||w||} \).

Points which are misclassified have a margin of \( \frac{1}{||w||} \).

\( (u)+ = \max(0,u) \)

Distance for class \( y_i = +1 \) points (red):
\( \xi_i = 1 - y_i(w^T x_i + b) \)

Distance for class \( y_i = -1 \) points (blue):
\( \xi_i = 1 + y_i(w^T x_i + b) \)

Convenient combined distance formula:
\( \xi_i = 1 - y_i(w^T x_i + b) \) for \( i = 1...n \)

we want the slab to be as large as possible while minimizing the number of misclassified points.

Optimization for the maximum margin hyperplane when the data is linearly separable: 
\( \min ||w||_2 \) subject to \( y_i(w^T x_i + b) - \xi_i \geq 1 \) for \( i = 1...n \)

we can simplify to the following:

\(\min (||w||_2\) + \(C \sum\) \( y_i(w^T x_i + b)_+ + \lambda||w||_2)\)

where \(\lambda\) is roughly equal to \( \frac{1}{C} \)

if the sum of the slacks is zero, none of the points are misclassified or within the hyperslab.

\((1 - y_i(w^T x_i + b))_+\) is the hinge loss function, which is the total slackness of the hyperslab (also known as the loss function).

Our aim is to minimize the hinge loss function and maximize the margin.

Tradeoff: driving the hinge loss down drives the margin up.

## Beyond Linear Separability


Up to now, we've assumed that SVM only works for linearly separable data.

Many real-world datasets are not linearly separable, so we can replace the \(n \times p\) matrix \(X\) with an \(n \times P\) data matrix \(\Phi(X)\) where \(\Phi\)'s columns are some non-linear function of the columns of \(X\).\

>\(x_i\) is replaced with \(\phi(x_i)\)

For example, adding a thrd quadratic term:
Replace \(x_i\) with \(\phi(x_i) = \begin{bmatrix} x_{i1} \\ x_{i2} \\ \sqrt{{x_{i}[1]}^2 + {x_{i}[2]}^2} \end{bmatrix}\)

The Stone-Weierstrass theorem states that while n data points cannot be linearly separated in \(p < n\) dimensions, they can be linearly separated in \(P \geq n\) dimensions.

Often, \(P >> p\), which significantly increases the costs of training and prediction.

The kernel trick allows us to avoid the explicit computation of \(\Phi(X)\) and \(||\Phi(X)||\).


## Summary

SVM: y = sgn(w^T x + b)

Optimize the angle and offset of a hyperplane to balance the hinge loss and margin (sloppiness and stability respectively).:

\[\min_{\mathbf{w} \in \mathbb{R}^p, b \in \mathbb{R}} C \sum 1 - y_i(\mathbf{w}^T \mathbf{x}_i + b)_+ + \lambda \| \mathbf{w} \|_2\]

Convex functions are amenalbe to optimization.