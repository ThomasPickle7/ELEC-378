# Artificial Neural Networks
## Turing Test
- The Turing Test is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.
- In the original illustrative example, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being.
- All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.
## AI
- Logic-Based AI
    - produced things like Lisp, KG, theroem provider
- Connectionist approach to AI
    - Neural Networks
    - Deep Learning
    - Focuses on the way the brain works

## Gartner Hype Cycle
- The Gartner Hype Cycle is a graphical presentation of the maturity, adoption, and social application of specific technologies.
- The hype cycle provides a cross-industry perspective on the technologies and trends that business strategists, chief innovation officers, R&D leaders, entrepreneurs, global market developers, and emerging-technology teams should consider in developing emerging-technology portfolios.
- The hype cycle has five phases:
    1. Technology Trigger
    2. Peak of Inflated Expectations
    3. Trough of Disillusionment
    4. Slope of Enlightenment
    5. Plateau of Productivity


## Linear Predictors
- A linear predictor is a linear function that maps the input to the output.
- given traininf data (x1, y1), (x2, y2), ..., (xn, yn), we want to find a linear function f(x) = w1x1 + w2x2 + ... + wnxn + b that predicts the output y for a given input x.
    - we do this via convex optimization to minimize the loss function and find the globally optimum parameters
## Non-lineaer Predictors
- A non-linear predictor is a non-linear function that maps the input to the output.
- Given training data (x1, y1), (x2, y2), ..., (xn, yn), we want to find a non-linear function \( \phi(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b \) that predicts the output y for a given input x.
    - We do this via convex optimization to minimize the loss function and find the globally optimum parameters
    - We studied polynomial kernels, Gaussian kernels, and the kernel trick
- Parameterize the non-linear function with a set of parameters and optimize the parameters to minimize the loss function
- The learnable nonlinear function warps the input space to make linear prediction easier
- The optimization objective function over the parameters is non-convex, so we use gradient descent to find the locally optimum parameters
## Perceptron
- A perceptron is a single-layer neural network that can learn linear functions.
- The perceptron is the simplest neural network, and it is the building block of more complex neural networks.

